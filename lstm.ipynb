{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Dataset can be downloaded from [here](https://www.kaggle.com/albenft/game-of-thrones-script-all-seasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            dialogue\n",
      "0  waymar royce: What do you expect? They're sava...\n",
      "1  will: I've never seen wildlings do a thing lik...\n",
      "2               waymar royce: How close did you get?\n",
      "3                      will: Close as any man would.\n",
      "4            gared: We should head back to the wall.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "script = pd.read_csv('./data/Game_of_Thrones_Script.csv')\n",
    "script['Name'] = script['Name'] + ': '\n",
    "script['dialogue'] = script['Name'] + script['Sentence']\n",
    "script.drop(['Release Date', 'Season', 'Episode', 'Episode Title', 'Name', 'Sentence'], axis=1, inplace=True)\n",
    "np.savetxt('./got_script.txt', script.values, fmt='%s')\n",
    "print(script.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "got_data_dir = './got_script.txt'\n",
    "input_file = os.path.join(got_data_dir)\n",
    "with open(input_file, \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "This cell will gives a sense of the data I'm be working with. For example, it is all lowercase text, and each new line of dialogue is separated by a newline character `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 21829\n",
      "Number of lines: 23912\n",
      "Average number of words in each line: 13.66782368685179\n",
      "\n",
      "The lines 0 to 10:\n",
      "waymar royce: What do you expect? They're savages. One lot steals a goat from another lot and before you know it, they're ripping each other to pieces.\n",
      "will: I've never seen wildlings do a thing like this. I've never seen a thing like this, not ever in my life.\n",
      "waymar royce: How close did you get?\n",
      "will: Close as any man would.\n",
      "gared: We should head back to the wall.\n",
      "royce: Do the dead frighten you?\n",
      "gared: Our orders were to track the wildlings. We tracked them. They won't trouble us no more.\n",
      "royce: You don't think he'll ask us how they died? Get back on your horse.\n",
      "will: Whatever did it to them could do it to us. They even killed the children.\n",
      "royce: It's a good thing we're not children. You want to run away south, run away. Of course, they will behead you as a deserter â€¦ If I don't catch you first. Get back on your horse. I won't say it again.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "view_line_range = (0, 10)\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup Table\n",
    "Since I'm using word embeddings, I'm transforming the words to ids by creating two dictionaries:\n",
    "\n",
    "Dictionary to go from the words to an id as `vocab_to_int`\n",
    "\n",
    "Dictionary to go from the id to word, we'll call `int_to_vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {i: word for i, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: i for i, word in int_to_vocab.items()}\n",
    "    return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Punctuation\n",
    "I'll be splitting the script into a word array using spaces as delimiters. However, punctuations like periods and exclamation marks can create multiple ids for the same word. For example, \"bye\" and \"bye!\" would generate two different word ids. \n",
    "\n",
    "The dictionary I create will be used to tokenize the symbols and add the delimiter (space) around it. This separates each symbols as its own word, making it easier for the neural network to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    tokens = dict()\n",
    "    tokens['.'] = '<period>'\n",
    "    tokens[','] = '<comma>'\n",
    "    tokens['\"'] = '<quotation>'\n",
    "    tokens[';'] = '<semicolon>'\n",
    "    tokens['!'] = '<exclamation>'\n",
    "    tokens['?'] = '<question>'\n",
    "    tokens['('] = '<left_parenthesis>'\n",
    "    tokens[')'] = '<right_parenthesis>'\n",
    "    tokens['-'] = '<dash>'\n",
    "    tokens['\\n'] = '<new_line>'\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "Calling the 2 functions created above on the data and saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "token_dict = token_lookup()\n",
    "for key, token in token_dict.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "int_text = [vocab_to_int[word] for word in text]\n",
    "pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_preprocess()` function will load the saved preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "The `batch_data()` function batches words data into chunks of size `batch_size` using the TensorDataset and DataLoader classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    n_batches = len(words) // batch_size\n",
    "    words = words[:n_batches * batch_size]\n",
    "    y_len = len(words) - sequence_length\n",
    "    x, y = [], []\n",
    "    for idx in range(0, y_len):\n",
    "        idx_end = sequence_length + idx\n",
    "        x_batch = words[idx:idx_end]\n",
    "        x.append(x_batch)\n",
    "        batch_y =  words[idx_end]\n",
    "        y.append(batch_y)    \n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
    "    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the batching\n",
    "I'm generating some test text data and defining a dataloader using the function defined above. Then, I'm getting some sample batch of inputs sample_x and targets sample_y from the dataloader. \n",
    "\n",
    "`sample_x` should have size (10, 5) and `sample_y` should have one dimension (10). \n",
    "\n",
    "Also, `sample_y` should be ordered such that it has the next value of each sequence of `test_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[25, 26, 27, 28, 29],\n",
      "        [ 2,  3,  4,  5,  6],\n",
      "        [13, 14, 15, 16, 17],\n",
      "        [39, 40, 41, 42, 43],\n",
      "        [32, 33, 34, 35, 36],\n",
      "        [ 3,  4,  5,  6,  7],\n",
      "        [23, 24, 25, 26, 27],\n",
      "        [19, 20, 21, 22, 23],\n",
      "        [17, 18, 19, 20, 21],\n",
      "        [26, 27, 28, 29, 30]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([30,  7, 18, 44, 37,  8, 28, 24, 22, 31])\n"
     ]
    }
   ],
   "source": [
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network\n",
    "The `__init__()` function creates the layers of the neural network and saves them to the class. The `forward()` function will use these layers to run forward propagation and generate an output and a hidden state.\n",
    "\n",
    "The output of this model is the last batch of word scores after a complete sequence has been processed, i.e, for each input sequence of words, the word scores for a single, most likely, next word is only the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        \"\"\"\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = nn_input.size(0)\n",
    "        embed = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embed, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1]\n",
    "        return out, hidden\n",
    "    \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM\n",
    "        '''\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backprop pass\n",
    "The `forward_back_prop()` function describes the forward and backprop steps by using the rnn defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda()\n",
    "    h = tuple([each.data for each in hidden])\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        inp, target = inp.cuda(), target.cuda()    \n",
    "    output, h = rnn(inp, h)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Process\n",
    "The training loop is implemented in the `train_rnn()` function. This function will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the show_every_n_batches parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    rnn.train()\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)\n",
    "            batch_losses.append(loss)\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 30\n",
    "batch_size = 64\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001 \n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 250\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "show_every_n_batches = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epoch(s)...\n",
      "Epoch:    1/10    Loss: 5.376764600594838\n",
      "\n",
      "Epoch:    1/10    Loss: 4.82786775747935\n",
      "\n",
      "Epoch:    1/10    Loss: 4.626995973904927\n",
      "\n",
      "Epoch:    1/10    Loss: 4.531740409851074\n",
      "\n",
      "Epoch:    2/10    Loss: 4.312917083774056\n",
      "\n",
      "Epoch:    2/10    Loss: 4.237606526056926\n",
      "\n",
      "Epoch:    2/10    Loss: 4.217242928345998\n",
      "\n",
      "Epoch:    2/10    Loss: 4.173193832556406\n",
      "\n",
      "Epoch:    3/10    Loss: 3.9802359445706164\n",
      "\n",
      "Epoch:    3/10    Loss: 3.9356480825742084\n",
      "\n",
      "Epoch:    3/10    Loss: 3.944112125873566\n",
      "\n",
      "Epoch:    3/10    Loss: 3.971533558209737\n",
      "\n",
      "Epoch:    4/10    Loss: 3.7491800918302376\n",
      "\n",
      "Epoch:    4/10    Loss: 3.73022225300471\n",
      "\n",
      "Epoch:    4/10    Loss: 3.7566925455729168\n",
      "\n",
      "Epoch:    4/10    Loss: 3.7741627855300903\n",
      "\n",
      "Epoch:    5/10    Loss: 3.552854811229224\n",
      "\n",
      "Epoch:    5/10    Loss: 3.552754388809204\n",
      "\n",
      "Epoch:    5/10    Loss: 3.598949054082235\n",
      "\n",
      "Epoch:    5/10    Loss: 3.628698653539022\n",
      "\n",
      "Epoch:    6/10    Loss: 3.408755863692926\n",
      "\n",
      "Epoch:    6/10    Loss: 3.410572488943736\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4709249645868936\n",
      "\n",
      "Epoch:    6/10    Loss: 3.505991451899211\n",
      "\n",
      "Epoch:    7/10    Loss: 3.283647470484492\n",
      "\n",
      "Epoch:    7/10    Loss: 3.299480989297231\n",
      "\n",
      "Epoch:    7/10    Loss: 3.362772660255432\n",
      "\n",
      "Epoch:    7/10    Loss: 3.3898699458440147\n",
      "\n",
      "Epoch:    8/10    Loss: 3.185402085357043\n",
      "\n",
      "Epoch:    8/10    Loss: 3.1978449714978536\n",
      "\n",
      "Epoch:    8/10    Loss: 3.2619454193115236\n",
      "\n",
      "Epoch:    8/10    Loss: 3.306713274161021\n",
      "\n",
      "Epoch:    9/10    Loss: 3.0936450344589947\n",
      "\n",
      "Epoch:    9/10    Loss: 3.1212568275133767\n",
      "\n",
      "Epoch:    9/10    Loss: 3.164861304283142\n",
      "\n",
      "Epoch:    9/10    Loss: 3.2363898337682087\n",
      "\n",
      "Epoch:   10/10    Loss: 3.020602551920961\n",
      "\n",
      "Epoch:   10/10    Loss: 3.0347638669013977\n",
      "\n",
      "Epoch:   10/10    Loss: 3.109920383930206\n",
      "\n",
      "Epoch:   10/10    Loss: 3.161385311126709\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjun/anaconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "save_filename = os.path.splitext(os.path.basename('./save/trained_rnn'))[0] + '.pt'\n",
    "torch.save(trained_rnn, save_filename)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    return torch.load(save_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "To generate the text, the network needs to start with a single word and repeat its predictions until it reaches a set length. The `generate()` function does exactly this. It takes a word id to start with, prime_id, and generates a set length of text, predict_len. Also, topk sampling is used to introduce some randomness in choosing the most likely next word, given an output set of word scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    rnn.eval()\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu()\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)\n",
    "        current_seq = current_seq.cpu()\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Fake Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will: the underlying region skull.\n",
      "davos: i didn't ask you, lord varys. i don't know how much it feels.\n",
      "shireen: i don't want to see the words.\n",
      "shireen: i'm not doing this.\n",
      "shireen: i'm not a smuggler.\n",
      "tyrion lannister: i don't know.\n",
      "tyrion lannister: you did. you don't want to see me.\n",
      "tyrion lannister: you must have taken your own hand.\n",
      "tyrion lannister: no, but you're making me. i didn't even want it to save my brother.\n",
      "tyrion lannister: and i intend to be a queen.\n",
      "daenerys targaryen: you will be happy about this.\n",
      "tyrion lannister: yes, i'm not a hero.\n",
      "tyrion lannister: you know how i know, you are.\n",
      "varys: i'm sorry.\n",
      "shae: you know who makes word about your father, your grace.\n",
      "tyrion lannister: i don't know where it is.\n",
      "varys: i do not recognize a ship.\n",
      "tyrion lannister: what? why do you know who i am?\n",
      "tyrion lannister: because i don't know. but i don't need to be alone. i was a whore.\n",
      "varys: i was invited to you.\n",
      "daenerys targaryen: and what does the people want for you?\n",
      "daario: no, i don't know.\n",
      "daenerys targaryen: you don't have to lecture the dothraki.\n",
      "daario: no one can protect me. and if you want them to execute me, i'll give you a cask.\n",
      "daenerys targaryen: you have a proper choice.\n",
      "tyrion lannister: i am not your queen.\n",
      "cersei lannister: and what do you want?\n",
      "tywin lannister: you don't need to see the king.\n",
      "cersei lannister: you should consider your personal interests to your sister.\n",
      "jaime lannister: you must. you don't understand me.\n",
      "jaime lannister: i don't understand.\n",
      "jaime lannister: you're going to miss me.\n",
      "myrcella: you are. you don't know that. you're not here to thank the people who know what you are.\n",
      "tyrion lannister: i don't know how many arrows to be.\n",
      "tyrion lannister: yes.\n",
      "shae: and how did you know\n"
     ]
    }
   ],
   "source": [
    "gen_length = 400\n",
    "prime_word = 'will'\n",
    "pad_word = '<PAD>'\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the fake script. \n",
    "f =  open(\"generated_script_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
