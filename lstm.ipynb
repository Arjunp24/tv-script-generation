{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = './data/Game_of_Thrones_Script.csv'\n",
    "input_file = os.path.join(data_path)\n",
    "with open(input_file, \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 37330\n",
      "Number of lines: 23913\n",
      "Average number of words in each line: 16.848492451804457\n",
      "\n",
      "The lines 0 to 10:\n",
      "Release Date,Season,Episode,Episode Title,Name,Sentence\n",
      "2011-04-17,Season 1,Episode 1,Winter is Coming,waymar royce,\"What do you expect? They're savages. One lot steals a goat from another lot and before you know it, they're ripping each other to pieces.\"\n",
      "2011-04-17,Season 1,Episode 1,Winter is Coming,will,\"I've never seen wildlings do a thing like this. I've never seen a thing like this, not ever in my life.\"\n",
      "2011-04-17,Season 1,Episode 1,Winter is Coming,waymar royce,How close did you get?\n",
      "2011-04-17,Season 1,Episode 1,Winter is Coming,will,Close as any man would.\n",
      "2011-04-17,Season 1,Episode 1,Winter is Coming,gared,We should head back to the wall.\n",
      "2011-04-17,Season 1,Episode 1,Winter is Coming,royce,Do the dead frighten you?\n",
      "2011-04-17,Season 1,Episode 1,Winter is Coming,gared,Our orders were to track the wildlings. We tracked them. They won't trouble us no more.\n",
      "2011-04-17,Season 1,Episode 1,Winter is Coming,royce,You don't think he'll ask us how they died? Get back on your horse.\n",
      "2011-04-17,Season 1,Episode 1,Winter is Coming,will,Whatever did it to them could do it to us. They even killed the children.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "view_line_range = (0, 10)\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = dict()\n",
    "    tokens['.'] = '<period>'\n",
    "    tokens[','] = '<comma>'\n",
    "    tokens['\"'] = '<quotation>'\n",
    "    tokens[';'] = '<semicolon>'\n",
    "    tokens['!'] = '<exclamation>'\n",
    "    tokens['?'] = '<question>'\n",
    "    tokens['('] = '<left_parenthesis>'\n",
    "    tokens[')'] = '<right_parenthesis>'\n",
    "    tokens['-'] = '<dash>'\n",
    "    tokens['\\n'] = '<new_line>'\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "token_dict = token_lookup()\n",
    "for key, token in token_dict.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "int_text = [vocab_to_int[word] for word in text]\n",
    "pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    \"\"\"\n",
    "    \n",
    "    n_batches = len(words) // batch_size\n",
    "    words = words[:n_batches * batch_size]\n",
    "    y_len = len(words) - sequence_length\n",
    "    x, y = [], []\n",
    "    for idx in range(0, y_len):\n",
    "        idx_end = sequence_length + idx\n",
    "        x_batch = words[idx:idx_end]\n",
    "        x.append(x_batch)\n",
    "        batch_y =  words[idx_end]\n",
    "        y.append(batch_y)    \n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
    "    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[37, 38, 39, 40, 41],\n",
      "        [ 8,  9, 10, 11, 12],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [25, 26, 27, 28, 29],\n",
      "        [40, 41, 42, 43, 44],\n",
      "        [27, 28, 29, 30, 31],\n",
      "        [18, 19, 20, 21, 22],\n",
      "        [16, 17, 18, 19, 20],\n",
      "        [24, 25, 26, 27, 28],\n",
      "        [33, 34, 35, 36, 37]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([42, 13, 10, 30, 45, 32, 23, 21, 29, 38])\n"
     ]
    }
   ],
   "source": [
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        \"\"\"\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = nn_input.size(0)\n",
    "        embed = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embed, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1]\n",
    "        return out, hidden\n",
    "    \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        '''\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda()\n",
    "    h = tuple([each.data for each in hidden])\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        inp, target = inp.cuda(), target.cuda()    \n",
    "    output, h = rnn(inp, h)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    rnn.train()\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)\n",
    "            batch_losses.append(loss)\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "batch_size = 128\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 0.001 \n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 250\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "show_every_n_batches = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "save_filename = os.path.splitext(os.path.basename('./save/trained_rnn))[0] + '.pt'\n",
    "torch.save(trained_rnn, save_filename)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    return torch.load(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    rnn.eval()\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu()\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_length = 400\n",
    "prime_word = 'jerry'\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  open(\"generated_script_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
